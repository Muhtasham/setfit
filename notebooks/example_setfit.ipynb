{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff54697",
   "metadata": {},
   "source": [
    "# SetFit Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86c7fe8",
   "metadata": {},
   "source": [
    "##### Hello! We will be going through a quick and easy example of using our setfit code for text classification. To complete this tutorial you will need dependencies from requirements.txt and SetFit installed in your environment. Please do so by running this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1deb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e079455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install setfit\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate==0.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296ef23",
   "metadata": {},
   "source": [
    "##### Before we proceed, we must first choose a model and identify the dataset we would like to classify. For this tutorial we'll be using SST2, already on the SetFit hub: \"SetFit/sst2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1512eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6557e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"SetFit/sst2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b17d9",
   "metadata": {},
   "source": [
    "##### We load the \"train\" and \"test\" portions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1c360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--sst2-4811211b52125821\n",
      "Reusing dataset json (/home/eun_seo_huggingface_co/.cache/huggingface/datasets/SetFit___json/SetFit--sst2-4811211b52125821/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    }
   ],
   "source": [
    "train_sst2 = load_dataset(dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5d9bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration SetFit--sst2-4811211b52125821\n",
      "Reusing dataset json (/home/eun_seo_huggingface_co/.cache/huggingface/datasets/SetFit___json/SetFit--sst2-4811211b52125821/0.0.0/da492aad5680612e4028e7f6ddc04b1dfcec4b64db470ed7cc5f2bb265b9b6b5)\n"
     ]
    }
   ],
   "source": [
    "test_sst2 = load_dataset(dataset,split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692bbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit.data import SAMPLE_SIZES, create_fewshot_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370e238",
   "metadata": {},
   "source": [
    "##### We not sample our data so that we have n number of examples for each class. We start with 4 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0901752",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "fewshot_sst2 = create_fewshot_splits(train_sst2, [n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155659c",
   "metadata": {},
   "source": [
    "##### Create_fewshot_splits has samples 10 different groups of n=4 (per class) data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d208a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-4-0\n",
      "train-4-1\n",
      "train-4-2\n",
      "train-4-3\n",
      "train-4-4\n",
      "train-4-5\n",
      "train-4-6\n",
      "train-4-7\n",
      "train-4-8\n",
      "train-4-9\n"
     ]
    }
   ],
   "source": [
    "for name in fewshot_sst2:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6253e",
   "metadata": {},
   "source": [
    "##### Let's try our SetFit test on just one run. We'll call it try1. This means we're training our model on just one run of 4 examples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23f8c3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'label_text'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try1 = 'train-4-0'\n",
    "fewshot_sst2[try1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df62820",
   "metadata": {},
   "source": [
    "##### We then download the backbone model, in this case \"paraphrase-mpnet-base-v2\" from Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecf6b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391af770",
   "metadata": {},
   "source": [
    "##### There are many loss options to choose from but for this simple example we'll use SupConLoss. We can import SupConLoss from setfit.modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c196bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit.modeling import SupConLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780a8f6",
   "metadata": {},
   "source": [
    "##### We split the data into train/test and text/labels parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da69aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fewshot_sst2[try1][\"text\"]\n",
    "y_train = fewshot_sst2[try1][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd6b643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_sst2[\"text\"]\n",
    "y_test = test_sst2[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691c523",
   "metadata": {},
   "source": [
    "##### We set the batch size to 16 and max sequence length to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51c3464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model.max_seq_length = 256\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4f3a4",
   "metadata": {},
   "source": [
    "##### Now we have to make our train data loader. For this, we'll import the DataLoader from pytorch, and helper functions from Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb6f667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers.datasets import SentenceLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea73bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [InputExample(texts=[text], label=label) for text, label in zip(x_train, y_train)]\n",
    "train_data_sampler = SentenceLabelDataset(train_examples)\n",
    "batch_size = min(batch_size, len(train_data_sampler))\n",
    "train_dataloader = DataLoader(train_data_sampler, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7e205fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_class = SupConLoss\n",
    "train_loss = loss_class(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08098fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_dataloader) * num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f3a5a",
   "metadata": {},
   "source": [
    "##### Set our warm-up steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1b1190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "warmup_steps = math.ceil(train_steps*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce484ed4",
   "metadata": {},
   "source": [
    "##### And we finally train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9b3a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=1,\n",
    "    steps_per_epoch=train_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    show_progress_bar=False,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ed17f",
   "metadata": {},
   "source": [
    "##### We are now ready to evaluate our model's performance. We'll be using accruacy as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60fdff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from setfit.modeling import SKLearnWrapper\n",
    "from evaluate import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d50e7a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"accuracy\"\n",
    "clf = SKLearnWrapper(model, LogisticRegression())\n",
    "metric_fn = load(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "305184b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "metrics = metric_fn.compute(predictions=y_pred, references=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1d6efc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8165842943437671}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
